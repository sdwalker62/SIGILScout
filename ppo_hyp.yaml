# PPO Hyperparameters
STEPS_PER_EPOCH: 4000
EPOCHS: 30
GAMMA: 0.99
CLIP_RATIO: 0.2
POLICY_LEARNING_RATE: 3.0e-4
VALUE_FUNCTION_LEARNING_RATE: 1.0e-3
TRAIN_POLICY_ITERATIONS: 80
TRAIN_VALUE_ITERATIONS: 80
LAM: 0.97
TARGET_KL: 0.01
HIDDEN_SIZES: !!python/tuple [64,64]

# True if you want to render the environment
RENDER: False